{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweet_scraping_GetOldtweet3_final.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOOVMwkq0aXEFu0v/z05K9k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccrivellari/INVEST_IO/blob/master/Tweet_scraping_GetOldtweet3_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYHNQEhOyx7X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "d7191bb4-eaef-4e94-a5ee-b8649e2c7a62"
      },
      "source": [
        "#import libraries\n",
        "\n",
        "!pip install GetOldTweets3\n",
        "import GetOldTweets3 as got\n",
        "import importlib\n",
        "from textblob import TextBlob\n",
        "import datetime as dt\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "#import google translate API to pass the tweet text and translate it in english.\n",
        "!pip install googletrans\n",
        "import googletrans\n",
        "from googletrans import Translator\n",
        "\n",
        "\n",
        "# re = python module for regular expression\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "consumer_key = 'Vzk6O5t967f1n4iz9iQcsQwhf'\n",
        "consumer_secret = 'Q8sncdY1j85mwNitOqb5Spcg6U6uuZ0E2KNHHZEYF2J4DdCyVi'\n",
        "access_token = '164224789-mFmU5YhessCNvubNojgfL605tbU0vh0NOna3HdQU'\n",
        "access_secret = 'Ic5LNOH2QqxziSq4eTQdJvbXo3noTcQDUzCKXgFcdPzpC'\n",
        "\n",
        "\n",
        "# Importing GetOldTweets3\n",
        "\n",
        "def get_tweets(username, top_only, start_date, end_date, max_tweets):\n",
        "   \n",
        "    # specifying tweet search criteria \n",
        "    tweetCriteria = got.manager.TweetCriteria().setUsername(username)\\\n",
        "                          .setTopTweets(top_only)\\\n",
        "                          .setSince(start_date)\\\n",
        "                          .setUntil(end_date)\\\n",
        "                          .setMaxTweets(max_tweets)\n",
        "    \n",
        "    # scraping tweets based on criteria\n",
        "    tweet = got.manager.TweetManager.getTweets(tweetCriteria)\n",
        "    \n",
        "    # creating list of tweets with the tweet attributes \n",
        "    # specified in the list comprehension\n",
        "    text_tweets = [[tw.date,\n",
        "                tw.id,\n",
        "                tw.username,\n",
        "                tw.permalink,\n",
        "                tw.text,\n",
        "                tw.hashtags,\n",
        "                tw.favorites,\n",
        "                tw.geo,\n",
        "                tw.retweets,\n",
        "                tw.mentions] for tw in tweet]\n",
        "    \n",
        "    # creating dataframe, assigning column names to list of\n",
        "    # tweets corresponding to tweet attributes\n",
        "    news_df = pd.DataFrame(text_tweets, \n",
        "                            columns = ['created_at', 'id', 'user', 'url', 'text', 'hashtags', 'favorite_count', 'location', 'retweet', 'user_mentions'])\n",
        "    \n",
        "    return news_df\n",
        "\n",
        "#2 SENTIMENT ANALYSIS\n",
        "# # # # TWITTER STREAM LISTENER - Analyzer # # # #\n",
        "# we can use TextBlob for tweet sentiment analysis and determine the polarity of tweet, from -3 to 3;\n",
        "# -3 = negative, 0 = neutral, 3 = positive\n",
        " \n",
        "class TweetAnalyzer():\n",
        "    \"\"\"\n",
        "    Functionality for analyzing and categorizing content from tweets.\n",
        "    \"\"\"\n",
        "    # claen_tweet using the re module to clean tweets text \n",
        "    def clean_tweet(self, text):\n",
        "        return str(' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", text).split()))\n",
        "    def translator(self, text):\n",
        "        translator = Translator()\n",
        "        translated_tweet = str(translator.translate(text))\n",
        "    def analyze_sentiment(self, text):\n",
        "        analysis = TextBlob(self.clean_tweet(text))\n",
        "        \n",
        "        if analysis.sentiment.polarity > 0:\n",
        "            return 3\n",
        "        elif analysis.sentiment.polarity == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return -3\n",
        "   "
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: GetOldTweets3 in /usr/local/lib/python3.6/dist-packages (0.0.11)\n",
            "Requirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from GetOldTweets3) (4.2.6)\n",
            "Requirement already satisfied: pyquery>=1.2.10 in /usr/local/lib/python3.6/dist-packages (from GetOldTweets3) (1.4.1)\n",
            "Requirement already satisfied: cssselect>0.7.9 in /usr/local/lib/python3.6/dist-packages (from pyquery>=1.2.10->GetOldTweets3) (1.1.0)\n",
            "Requirement already satisfied: googletrans in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.6/dist-packages (from googletrans) (0.13.3)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (0.9.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (3.0.4)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (1.4.0)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (2020.9.15)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (2.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (2020.6.20)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (1.1.0)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.6/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.6/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (3.2.0)\n",
            "Requirement already satisfied: contextvars>=2.1; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from sniffio->httpx==0.13.3->googletrans) (2.4)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.6/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (3.0.0)\n",
            "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars>=2.1; python_version < \"3.7\"->sniffio->httpx==0.13.3->googletrans) (0.14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ePL3kWzIe_7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "ede7521c-d40d-47cc-cd76-59acefa7574d"
      },
      "source": [
        "# Defining news sources I want to include\n",
        "news_sources = \"$AAPL\"\n",
        "\n",
        "# news_sources = ['$AAPL', '$AMZN', '$MSFT', '$GOOGL', '$FB', '$BABA', '$0700', '$BRK', '$V', '$WMT', '$JNJ', '$TSM', '$TSLA', '$PG']\n",
        "# getting tweets from the defined new sources, \n",
        "# only including top tweets, \n",
        "# looking at the past week with the end_date not inclusive,\n",
        "# and specifying that we want a max number of tweets = 100.\n",
        "# also sorting the tweets by date, descending.\n",
        "\n",
        "initial_date = \"2020-01-01\"\n",
        "final_date = \"2020-09-15\"\n",
        "max_tweet = 10000  #per keyword\n",
        "\n",
        "news_df = get_tweets(news_sources, top_only = True, start_date = initial_date, end_date = final_date, max_tweets = max_tweet).sort_values('created_at', ascending=False)\n",
        "\n",
        "\n",
        "# getting the tweet full text \n",
        "pd.set_option(\"display.max_colwidth\", -1)\n",
        "\n",
        "# adding the new items 'date' and 'ticker'\n",
        "ticker = str(news_sources)\n",
        "\n",
        "news_df[\"date\"] = news_df[\"created_at\"].dt.strftime(\"%Y-%m-%d\")\n",
        "news_df[\"ticker\"] = f'{news_sources.replace(\"$\", \"\")}'\n",
        "\n",
        "# call the class TweetAnalyzer before fill in the column 'sentiment'\n",
        "tweet_analyzer = TweetAnalyzer()\n",
        "\n",
        "# ETL on tweets: cleaning, translate, sentiment analysis:\n",
        "\n",
        "translator = Translator()\n",
        "news_df['cleaned_text'] = np.array([tweet_analyzer.clean_tweet(text) for text in news_df['text']])\n",
        "news_df['translated_text'] = np.array([translator.translate(text) for text in news_df['cleaned_text']])\n",
        "\n",
        "#text language detection\n",
        "\n",
        "news_df['dect_lan'] = np.array([translator.detect(text) for text in news_df['cleaned_text']])\n",
        "news_df['dect_lan'] = str(news_df['dect_lan'])[19:21].upper()\n",
        "\n",
        "#sentiment analysis\n",
        "\n",
        "polarity = lambda x: TextBlob(x).sentiment.polarity\n",
        "subjectivity = lambda x: TextBlob(x).sentiment.subjectivity\n",
        "\n",
        "news_df['polarity'] = news_df['text'].astype(str).apply(polarity)\n",
        "news_df['subjectivity'] = news_df['text'].astype(str).apply(subjectivity)\n",
        "\n",
        "tweet_analyzer = TweetAnalyzer()\n",
        "\n",
        "news_df['sentiment_analysis'] = np.array([tweet_analyzer.analyze_sentiment(text) for text in news_df['text']])\n",
        "\n",
        "#salvataggio file csv con stringa del TS convertito in data\n",
        "\n",
        "news_df = news_df[['date', 'ticker', 'user', 'url', 'created_at', 'id', 'text', 'hashtags', 'translated_text', 'dect_lan', 'favorite_count', 'location', 'retweet', 'user_mentions', 'polarity', 'subjectivity', 'sentiment_analysis']]\n",
        "#news_df = news_df[['date', 'ticker', 'user', 'link', 'created_at', 'id', 'text', 'hashtags', 'favorite_count', 'location', 'retweet', 'user_mentions', 'polarity', 'subjectivity', 'sentiment_analysis']]\n",
        "\n",
        "news_df.to_csv(f'tweet_{initial_date.replace(\"-\", \"\")}' + \"_\" + f'{final_date.replace(\"-\", \"\")}' + \"_\" +  f'{news_sources.replace(\"$\" , \"\")}' + \".csv\", index= False)\n",
        "#files.download(f'tweet_{initial_date.replace(\"-\", \"\")}' + \"_\" + f'{final_date.replace(\"-\", \"\")}' + \"_\" +  f'{news_sources.replace(\"$\" , \"\")}' + \".csv\")\n",
        "news_df.head(100)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "An error occured during an HTTP request: HTTP Error 404: Not Found\n",
            "Try to open in browser: https://twitter.com/search?q=%20from%3A%24aapl%20since%3A2020-01-01%20until%3A2020-09-15&src=typd\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccyTMoLpVvNY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "06f34f38-471d-4140-de6b-b9df68dea1f3"
      },
      "source": [
        "# Plotting\n",
        "\n",
        "news_df['sentiment_analysis'].value_counts().plot(kind = 'bar')\n",
        "plt.show()"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAN10lEQVR4nO3df4xl9V2H8ectC0RpFXBHRGAdqNsaMHXbTtCktkFrFYqBogmyMZW2xIWkJDaaKK2JNMYmWIsk/qJZZANNWiiKWBKw7UqaEpJSmaVkWQqUHy5hN8vutJhSpaHd3Y9/7Bl7O8x2Zu65dy5893klk7n3e86955NM8uTO2XNmU1VIktryI5MeQJI0esZdkhpk3CWpQcZdkhpk3CWpQWsmPQDA2rVra3p6etJjSNKryrZt275RVVOLbXtFxH16eprZ2dlJjyFJrypJnjncNk/LSFKDjLskNci4S1KDjLskNWjJuCfZkmRfkh0Da59J8lD3tTPJQ936dJLvDGz7xDiHlyQtbjlXy9wE/D3wyfmFqvrd+cdJrgW+NbD/U1W1YVQDSpJWbsm4V9W9SaYX25YkwMXAr412LElSH33Pub8N2FtVTwysnZ7kq0m+lORtPd9fkjSEvjcxbQRuGXi+B1hXVd9M8hbg35KcVVUvLHxhkk3AJoB169b1HEOSNGjouCdZA/w28Jb5tap6CXipe7wtyVPA64GX3X5aVZuBzQAzMzOr+j+GTF9112oebtXtvOb8SY8gacL6nJb5deCxqto1v5BkKslR3eMzgPXA0/1GlCSt1HIuhbwF+DLwhiS7klzWbbqEHzwlA/B2YHt3aeS/AFdU1fOjHFiStLTlXC2z8TDr711k7Xbg9v5jSZL68A5VSWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWrQknFPsiXJviQ7BtY+kmR3koe6r3cNbPtQkieTPJ7kN8c1uCTp8Jbzyf0m4NxF1q+rqg3d190ASc4ELgHO6l7zj0mOGtWwkqTlWTLuVXUv8Pwy3+9C4Naqeqmq/gt4Eji7x3ySpCH0Oed+ZZLt3WmbE7q1U4BnB/bZ1a29TJJNSWaTzM7NzfUYQ5K00LBxvx54HbAB2ANcu9I3qKrNVTVTVTNTU1NDjiFJWsxQca+qvVV1oKoOAjfw/VMvu4HTBnY9tVuTJK2ioeKe5OSBpxcB81fS3AlckuTYJKcD64H/7DeiJGml1iy1Q5JbgHOAtUl2AVcD5yTZABSwE7gcoKoeSXIb8DVgP/CBqjowntElSYezZNyrauMiyzf+kP0/Cny0z1CSpH68Q1WSGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGrRk3JNsSbIvyY6Btb9O8liS7UnuSHJ8tz6d5DtJHuq+PjHO4SVJi1vOJ/ebgHMXrG0FfqGq3gh8HfjQwLanqmpD93XFaMaUJK3EknGvqnuB5xesfaGq9ndP7wdOHcNskqQhjeKc+/uBfx94fnqSryb5UpK3He5FSTYlmU0yOzc3N4IxJEnzesU9yZ8B+4FPdUt7gHVV9Sbgj4BPJ/nxxV5bVZuraqaqZqampvqMIUlaYOi4J3kv8FvA71VVAVTVS1X1ze7xNuAp4PUjmFOStAJDxT3JucCfABdU1YsD61NJjuoenwGsB54exaCSpOVbs9QOSW4BzgHWJtkFXM2hq2OOBbYmAbi/uzLm7cBfJPkecBC4oqqeX/SNJUljs2Tcq2rjIss3Hmbf24Hb+w4lSerHO1QlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHLinuSLUn2JdkxsHZikq1Jnui+n9CtJ8nfJnkyyfYkbx7X8JKkxS33k/tNwLkL1q4C7qmq9cA93XOA84D13dcm4Pr+Y0qSVmJZca+qe4HnFyxfCNzcPb4ZePfA+ifrkPuB45OcPIphJUnL0+ec+0lVtad7/BxwUvf4FODZgf12dWs/IMmmJLNJZufm5nqMIUlaaM0o3qSqKkmt8DWbgc0AMzMzK3qtjmzTV9016RHGauc15096BDWgzyf3vfOnW7rv+7r13cBpA/ud2q1JklZJn7jfCVzaPb4U+OzA+u93V838MvCtgdM3kqRVsKzTMkluAc4B1ibZBVwNXAPcluQy4Bng4m73u4F3AU8CLwLvG/HMkqQlLCvuVbXxMJvesci+BXygz1CSpH68Q1WSGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBy/oPsheT5A3AZwaWzgD+HDge+ANgrlv/cFXdPfSEkqQVGzruVfU4sAEgyVHAbuAO4H3AdVX18ZFMKElasVGdlnkH8FRVPTOi95Mk9TCquF8C3DLw/Mok25NsSXLCiI4hSVqm3nFPcgxwAfDP3dL1wOs4dMpmD3DtYV63Kclsktm5ubnFdpEkDWkUn9zPAx6sqr0AVbW3qg5U1UHgBuDsxV5UVZuraqaqZqampkYwhiRp3ijivpGBUzJJTh7YdhGwYwTHkCStwNBXywAkOQ54J3D5wPLHkmwACti5YJskaRX0intV/S/wkwvW3tNrIklSb96hKkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1KBe/1mHJK3E9FV3TXqEsdp5zfmTHuH/+cldkhpk3CWpQcZdkhpk3CWpQb3/QTXJTuDbwAFgf1XNJDkR+AwwDewELq6q/+57LEnS8ozqk/uvVtWGqprpnl8F3FNV64F7uueSpFUyrtMyFwI3d49vBt49puNIkhYxirgX8IUk25Js6tZOqqo93ePngJMWvijJpiSzSWbn5uZGMIYkad4obmL6laraneSngK1JHhvcWFWVpBa+qKo2A5sBZmZmXrZdkjS83p/cq2p3930fcAdwNrA3yckA3fd9fY8jSVq+XnFPclyS184/Bn4D2AHcCVza7XYp8Nk+x5EkrUzf0zInAXckmX+vT1fV55I8ANyW5DLgGeDinseRJK1Ar7hX1dPALy6y/k3gHX3eW5I0PO9QlaQGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGDR33JKcl+WKSryV5JMkfdusfSbI7yUPd17tGN64kaTnW9HjtfuCPq+rBJK8FtiXZ2m27rqo+3n88SdIwho57Ve0B9nSPv53kUeCUUQ0mSRreSM65J5kG3gR8pVu6Msn2JFuSnHCY12xKMptkdm5ubhRjSJI6veOe5DXA7cAHq+oF4HrgdcAGDn2yv3ax11XV5qqaqaqZqampvmNIkgb0inuSozkU9k9V1b8CVNXeqjpQVQeBG4Cz+48pSVqJPlfLBLgReLSq/mZg/eSB3S4Cdgw/niRpGH2ulnkr8B7g4SQPdWsfBjYm2QAUsBO4vNeEkqQV63O1zH1AFtl09/DjSJJGwTtUJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBY4t7knOTPJ7kySRXjes4kqSXG0vckxwF/ANwHnAmsDHJmeM4liTp5cb1yf1s4MmqerqqvgvcClw4pmNJkhZYM6b3PQV4duD5LuCXBndIsgnY1D39nySPj2mWV4K1wDdW62D5q9U60hHDn9+rV+s/u5893IZxxX1JVbUZ2Dyp46+mJLNVNTPpOTQcf36vXkfyz25cp2V2A6cNPD+1W5MkrYJxxf0BYH2S05McA1wC3DmmY0mSFhjLaZmq2p/kSuDzwFHAlqp6ZBzHepU4Ik4/Ncyf36vXEfuzS1VNegZJ0oh5h6okNci4S1KDjLskNWhi17m3LMnZQFXVA92fXTgXeKyq7p7waFpCkp/n0N3Up3RLu4E7q+rRyU0lrZz/oDpiSa7m0N/UWQNs5dCduV8E3gl8vqo+OsHx9EMk+VNgI4f+XMaubvlUDl3Ke2tVXTOp2TScJCdW1fOTnmMSjPuIJXkY2AAcCzwHnFpVLyT5UeArVfXGiQ6ow0rydeCsqvregvVjgEeqav1kJtNyJHkr8E/AQeD9wF8CZwDHABdX1ZcnON6q87TM6O2vqgPAi0meqqoXAKrqO0kOTng2/XAHgZ8BnlmwfnK3Ta9s1wEXA68B7gLeXVX3JXkz8HfAWyc53Goz7qP33SQ/VlUvAm+ZX0zyExiIV7oPAvckeYLv/+G7dcDPAVdObCot19FV9TBAkrmqug+gqh7sfnM+ohj30Xt7Vb0EUFWDMT8auHQyI2k5qupzSV7PoT9ZPfgPqg90v43plW3w6r8PLdh2zGoO8krgOXdJTUhyAfAf3W/N82s/DRwH/E5VfWxiw02AcZfUrCQPVtWbJz3HJHgTk6SWZdIDTIpxl9SyGyY9wKR4WkaSGuQnd0lqkHGXpAYZd0lqkHGXpAb9H2A0ZP4SDFqSAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ3AxiWpcmoM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "1217541e-245e-4c18-da75-fb1fadf71853"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#Create the wordcloud with the text created above\n",
        "wordcloud = WordCloud().generate(text)\n",
        "\n",
        "#Plot the text with the lines below\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-9ad4473f0679>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Create the wordcloud with the text created above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mwordcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#Plot the text with the lines below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zYLzgy7lDni",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "e8392eaa-42b0-4e00-c47f-3d8e08c9962f"
      },
      "source": [
        "df_news['sentiment_analysis']"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-d2e9ed0d6411>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_news\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment_analysis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_news' is not defined"
          ]
        }
      ]
    }
  ]
}